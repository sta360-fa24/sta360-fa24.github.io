{
  "hash": "1a3931524edb4789196b37ba1b41b732",
  "result": {
    "markdown": "---\ntitle: \"Easy Bayesian linear modeling\"\nformat: \n    revealjs:\n      mainfont: Lato\n      smaller: true\n---\n\n\n# `rstanarm` and `bayesplot`\n\n## Download\n\nTo download `rstanarm` and `bayesplot` run the code below\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"rstanarm\", \"bayesplot\")\n```\n:::\n\n\nTo load the package, run \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\nlibrary(bayesplot)\n```\n:::\n\n\n\n## Overview\n\n- `rstanarm` contains a host of functions to make Bayesian linear modeling in R easy. See [https://mc-stan.org/rstanarm/articles/](https://mc-stan.org/rstanarm/articles/) for a variety of tutorials.\n\n  - pros: fast and easy to test Bayesian linear models\n  \n  - cons: limited in scope, e.g. requires differentiable objective and small model adjustments can be cumbersome to implement, e.g. placing a  prior on variance versus standard deviation of normal model.\n\n- `bayesplot` contains many useful plotting wrappers that work out of the box with objects created by `rstanarm` in an intuitive way.\n\n## Example\n\n::: panel-tabset\n\n## code\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nbass = read_csv(\n     \"https://sta101-fa22.netlify.app/static/appex/data/bass.csv\")\nglimpse(bass)\n```\n:::\n\n\n## output\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 171\nColumns: 5\n$ river   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ station <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ length  <dbl> 47.0, 48.7, 55.7, 45.2, 44.7, 43.8, 38.5, 45.8, 44.0, 40.4, 47…\n$ weight  <dbl> 1616, 1862, 2855, 1199, 1320, 1225, 870, 1455, 1220, 1033, 337…\n$ mercury <dbl> 1.60, 1.50, 1.70, 0.73, 0.56, 0.51, 0.48, 0.95, 1.40, 0.50, 0.…\n```\n:::\n:::\n\n\n:::\n\n**Description** \n\nMercury, is a naturally occurring element that can have toxic effects on the nervous, digestive and immune systems of humans.\nIn local rivers microbes transform mercury into the highly toxic methyl mercury. Fish accumulate methyl mercury (since they are unable to excrete it) in their tissue over the course of their life.\n\nBass from the Waccamaw and Lumber Rivers were caught randomly, weighed, and measured. In addition, a filet from each fish caught was sent to the lab so that the tissue concentration of mercury could be determined for each fish. Each fish caught corresponds to a single row of the data frame.\nA code book is provided below.\n\n- river: 0=Lumber, 1=Waccamaw\n- station that the fish was collected at\n- length of the fish in centimeters\n- weight of the fish in grams\n- mercury: concentration of mercury in parts per million (ppm)\n\nThe data come from Craig Stowe, Nicholas School of the Environment circa 1990s\n\n## Fitting a normal linear model\n\nLet's consider the model below:\n\n\n$$\ny_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon_i\n$$\n\n\nwhere $\\epsilon_i \\sim \\text{ i.i.d.} N(0, \\sigma^2)$. We complete model specification with the priors:\n\n\n$$\n\\begin{aligned}\n\\beta_0 &\\sim N(0, 100)\\\\\n\\beta_i &\\sim \\text{ i.i.d } N(0, 1) \\text{ for all } i > 0\\\\\n\\sigma &\\sim \\text{uniform}(0, \\infty)\n\\end{aligned}\n$$\n\nLet's look at building this model using the `stan_glm` function of `rstanarm`.\n\n:::callout-note\nWe'll always want to access the object we create, so you should save the result, e.g. `model1` below.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save the result as \"model1\"\nmodel1 = stan_glm(mercury ~ ., data = bass, # remove the intercept\n                 family = gaussian(link = \"identity\"),\n                 seed = 360, # sets a random starting seed\n                 prior_intercept = normal(0, 100), # sets the intercept prior\n                 prior = normal(0, 1), # sets the beta prior\n                 prior_aux = NULL, # set a flat prior on sigma\n                 ) \n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Examining the output\n\n- Did `stan_glm` do what we think it did? Did the Markov chain converge?\n\n::: panel-tabset\n\n## quick look\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      mercury ~ .\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 171\n predictors:   5\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) -1.4    0.3 -1.9  -1.4  -1.0 \nriver       -0.5    0.2 -0.8  -0.5  -0.3 \nstation      0.1    0.0  0.1   0.1   0.1 \nlength       0.1    0.0  0.0   0.1   0.1 \nweight       0.0    0.0  0.0   0.0   0.0 \nsigma        0.6    0.0  0.5   0.6   0.6 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 1.2    0.1  1.1   1.2   1.3  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2864 \nriver         0.0  1.0  1857 \nstation       0.0  1.0  1828 \nlength        0.0  1.0  2814 \nweight        0.0  1.0  2861 \nsigma         0.0  1.0  2819 \nmean_PPD      0.0  1.0  3069 \nlog-posterior 0.0  1.0  1616 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n:::\n:::\n\n\n## check priors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPriors for model 'model1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 100)\n\nCoefficients\n ~ normal(location = [0,0,0,...], scale = [1,1,1,...])\n\nAuxiliary (sigma)\n ~ flat\n------\nSee help('prior_summary.stanreg') for more details\n```\n:::\n:::\n\n## trace plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_trace(model1)\n```\n\n::: {.cell-output-display}\n![](lab8-rstan_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n## marginal posteriors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_hist(model1)\n```\n\n::: {.cell-output-display}\n![](lab8-rstan_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n## plotting tips\n\nTo plot specific parameters, use the arguemnt `pars`, e.g.\n\n- `mcmc_trace(model1, pars = c(\"river\", \"station\")`\n- `mcmc_hist(model1, pars = \"length\")`\n\nTo read more about `bayesplot` functionality, see [https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html](https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html)\n\n## residual plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = data.frame(yhat = model1$fitted.values,\n                residual = model1$residuals)\n\ndf %>%\n  ggplot(aes(x = yhat, y = residual)) +\n  geom_point() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](lab8-rstan_files/figure-revealjs/unnamed-chunk-12-1.png){width=480}\n:::\n:::\n\n## get chain\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchain_draws = as_draws(model1)\nprint(names(chain_draws))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"(Intercept)\" \"river\"       \"station\"     \"length\"      \"weight\"     \n[6] \"sigma\"       \".chain\"      \".iteration\"  \".draw\"      \n```\n:::\n\n```{.r .cell-code}\nchain_draws$river[1:5] # first 5 samples of the first chain run by stan\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.3748586 -0.2472218 -0.8788921 -0.4870408 -0.6565987\n```\n:::\n:::\n\n\n- try the following command: `View(chain_draws)`\n\n## summarize\n\nReport posterior mean, posterior median and 90% posterior CI.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposteriorMean = apply(chain_draws[,1:6], 2, mean)\nposteriorMedian = model1$coefficients\nposteriorCI = posterior_interval(model1, prob = 0.9)\ncbind(posteriorMean, posteriorMedian, posteriorCI)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            posteriorMean posteriorMedian            5%           95%\n(Intercept) -1.4320242215    -1.435098280 -2.0129526891 -8.767706e-01\nriver       -0.5368486696    -0.538925449 -0.8441961657 -2.299097e-01\nstation      0.0777090920     0.078115486  0.0461303215  1.092244e-01\nlength       0.0622405968     0.062062419  0.0429862421  8.203404e-02\nweight      -0.0001059707    -0.000104737 -0.0002995203  8.105592e-05\nsigma        0.5568710635    -1.435098280  0.5088456218  6.111942e-01\n```\n:::\n:::\n\n:::\n\n## Exercise: Bayesian logistic regression\n\nUsing the `bass` data set, see how well you can predict which river a bass came from, given only its length and mercury level. In other words,\n\n\n$$\np(y_i = 1) = \\frac{1}{1 + \\exp [- (\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)]}\n$$\n\nUse the priors\n\n\n$$\n\\beta_i \\sim N(0, 4) \\text{ for } i \\in (0, 1, 2)\n$$\n\n\nA template for fitting logistic regression using `rstanarm` can be found at [https://mc-stan.org/rstanarm/articles/binomial.html](https://mc-stan.org/rstanarm/articles/binomial.html).\n\n- Does your chain converge?\n- Report $E[\\beta_i | y]$ and 80% posterior CI for each $\\beta$, i.e. $i \\in (0, 1, 2)$\n- Is `length` or `mercury` a more important predictor of which river the bass came from? Why?\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "lab8-rstan_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}